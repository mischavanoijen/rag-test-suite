# CrewAI Test Suite â€” Project Status

*Last Updated: January 12, 2026 (15:30 UTC)*

---

## Executive Summary

The **rag-test-suite** is a CrewAI Flow that automatically tests RAG-based chat crews. The implementation is **COMPLETE** and **INTEGRATION TESTED** â€” all core components are built, 145/145 unit tests pass, and the test suite has been validated against a live RAG Engine MCP (Market Intelligence corpus).

### Recent Updates
- âœ… **Integration tested** against live MI RAG Engine MCP
- âœ… **Discovery mode** successfully mapped RAG knowledge domains
- âœ… **Test generation** created high-quality test cases from real RAG content
- Regenerated `uv.lock` for deployment
- Added comprehensive README.md and integration test plan
- Created integration test scripts

---

## Implementation Status

| Component | Status | Notes |
|-----------|--------|-------|
| **Project Scaffold** | âœ… Complete | Standard CrewAI Flow structure with pyproject.toml |
| **Models & State** | âœ… Complete | Pydantic models for all data structures |
| **Configuration Loader** | âœ… Complete | YAML config with environment variable overrides |
| **Discovery Crew** | âœ… Complete | Queries RAG to map knowledge domains |
| **Prompt Generator Crew** | âœ… Complete | Generates agent/prompt suggestions from RAG data |
| **Test Generation Crew** | âœ… Complete | Creates test cases with expected answers |
| **Execution Flow** | âœ… Complete | Multi-mode flow with CSV import support |
| **Evaluation Crew** | âœ… Complete | Analyzes results and patterns |
| **Reporting Crew** | âœ… Complete | Generates markdown quality reports |
| **RagQueryTool** | âœ… Complete | RAG Engine MCP + Qdrant backends |
| **CrewRunnerTool** | âœ… Complete | API mode (Enterprise) + Local mode |
| **EvaluatorTool** | âœ… Complete | LLM-as-judge evaluation |
| **Unit Tests** | âœ… Complete | 145 tests, all passing |
| **Integration Testing** | âœ… Complete | Tested with MI RAG Engine MCP |
| **CrewAI Enterprise Deploy** | ðŸ”„ Pending | Ready for deployment |

---

## Run Modes

The test suite supports multiple execution modes:

| Mode | Description | Use Case |
|------|-------------|----------|
| `full` | Discovery â†’ Prompts â†’ Tests â†’ Execute â†’ Report | Complete test cycle |
| `prompt_only` | Discovery â†’ Prompt suggestions only | Get agent configuration recommendations |
| `generate_only` | Discovery â†’ Prompts â†’ Test cases (no execution) | Create test cases without running them |
| `execute_only` | Load tests from CSV â†’ Execute â†’ Report | Re-run specific test sets |
| `generate_and_execute` | Same as `full` | Default behavior |

---

## Key Files

```
rag-test-suite/
â”œâ”€â”€ README.md                  # Usage documentation
â”œâ”€â”€ pyproject.toml             # Project configuration
â”œâ”€â”€ uv.lock                    # Dependency lock file
â”œâ”€â”€ .env.example               # Environment variable template
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ sample_tests.csv       # Example test cases
â””â”€â”€ src/rag_test_suite/
â”œâ”€â”€ flow.py                    # Main Flow orchestration (RAGTestSuiteFlow)
â”œâ”€â”€ main.py                    # CLI entry point + CrewAI Enterprise endpoints
â”œâ”€â”€ models.py                  # Pydantic models (TestCase, TestResult, etc.)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ loader.py              # YAML config loader with env overrides
â”‚   â””â”€â”€ settings.yaml          # Default configuration
â”œâ”€â”€ crews/
â”‚   â”œâ”€â”€ discovery/             # RAG knowledge discovery
â”‚   â”œâ”€â”€ prompt_generator/      # Agent/prompt suggestions
â”‚   â”œâ”€â”€ test_generation/       # Test case creation
â”‚   â”œâ”€â”€ evaluation/            # Result analysis
â”‚   â””â”€â”€ reporting/             # Report generation
â””â”€â”€ tools/
    â”œâ”€â”€ rag_query.py           # RAG query tool (MCP + Qdrant)
    â”œâ”€â”€ crew_runner.py         # Target crew execution (API + local)
    â””â”€â”€ evaluator.py           # LLM-as-judge evaluation
```

---

## Environment Variables

### Required for RAG Engine (MCP)

| Variable | Description |
|----------|-------------|
| `PG_RAG_MCP_URL` | RAG Engine MCP server URL |
| `PG_RAG_TOKEN` | Bearer token for MCP authentication |
| `PG_RAG_CORPUS` | Corpus name/path for RAG queries |

### Required for Target Crew (API Mode)

| Variable | Description |
|----------|-------------|
| `TARGET_API_URL` | CrewAI Enterprise kickoff URL |
| `TARGET_API_TOKEN` | Bearer token for Enterprise API |

### Required for LLM

| Variable | Description |
|----------|-------------|
| `OPENAI_API_KEY` | API key for LiteLLM proxy |
| `OPENAI_API_BASE` | LiteLLM proxy base URL |

---

## Usage Examples

### CLI Usage

```bash
# Generate prompt suggestions only
python -m rag_test_suite.main --run-mode prompt_only

# Generate test cases without executing
python -m rag_test_suite.main --run-mode generate_only --num-tests 10

# Execute tests from CSV file
python -m rag_test_suite.main --run-mode execute_only --test-csv tests.csv

# Full test run with API target
python -m rag_test_suite.main \
  --run-mode full \
  --target-api-url https://app.crewai.com/api/v1/crews/123/kickoff \
  --crew-description "Customer support assistant"
```

### Programmatic Usage

```python
from rag_test_suite.flow import run_flow

result = run_flow(
    target_api_url="https://app.crewai.com/api/v1/crews/123/kickoff",
    num_tests=20,
    crew_description="Customer support assistant for retail queries",
    run_mode="full",
)
```

---

## Test Results

```
$ python -m pytest tests/ -v
============================= 145 passed in 1.72s =============================
```

All tests passing:
- Configuration loader tests
- Model validation tests
- Tool unit tests (RagQueryTool, CrewRunnerTool, EvaluatorTool)
- Crew initialization tests (Discovery, TestGeneration, Evaluation, Reporting, PromptGenerator)
- Flow state and routing tests

---

## Integration Test Results

### Test Environment

- **RAG Backend:** MI RAG Engine MCP (`https://kon-mcp-ragengine-xzxewckhqa-ez.a.run.app`)
- **Corpus:** Market Intelligence documents (Gartner, Everest Group, TBR reports)
- **Date:** January 12, 2026

### Phase 1: RAG Connectivity âœ…

```
âœ“ MCP URL: https://kon-mcp-ragengine-xzxewckhqa-ez.a.run.app
âœ“ Connection established
âœ“ Query returned results with relevance scores
```

Sample query "What is BPO?" returned relevant results from IT Services and Customer Experience documents.

### Phase 2: Discovery Mode (prompt_only) âœ…

The Discovery Crew successfully:
- Mapped knowledge domains: CXM, Data & AI, Competitive Insights, Digital Solutions, BFSI
- Generated agent configuration: "Customer Experience & Support Strategist"
- Created system prompt with proper boundaries
- Identified out-of-scope examples (Ancient Roman history, etc.)

### Phase 3: Test Generation (generate_only) âœ…

Generated 5 high-quality test cases:

| ID | Category | Difficulty | Question Preview |
|----|----------|------------|------------------|
| TEST-001 | factual | easy | "What is the definition of composable customer service?" |
| TEST-002 | reasoning | medium | "How might Generative AI capabilities influence CXM by 2028?" |
| TEST-003 | out_of_scope | hard | "Explain the historical significance of the Roman Empire..." |
| TEST-004 | ambiguous | medium | "What are the key competitive factors in the market..." |
| TEST-005 | reasoning | medium | "How are Cloud and Data & AI driving business success..." |

All test cases include expected answers derived from RAG content.

### Phase 4: Execution â¸ï¸ Pending

Requires a deployed target crew (e.g., simple-rag with MI RAG configuration). See `docs/INTEGRATION_TEST_PLAN.md` for setup instructions.

---

## Next Steps

### Immediate (Complete Execution Testing)

1. **Deploy simple-rag** â€” Configure with MI RAG credentials
2. **Run execute_only mode** â€” Test with generated CSV
3. **Run full mode** â€” Complete end-to-end test cycle
4. **Validate reports** â€” Review generated quality reports

### Deployment (CrewAI Enterprise)

1. **Push to GitHub** â€” Commit all changes (lock file already regenerated)
2. **Create flow in Studio** â€” Link GitHub repo to CrewAI Enterprise
3. **Configure environment** â€” Add required env vars in Studio Settings
4. **Test kickoff** â€” Trigger via API or Studio UI

### Enhancements (Future)

1. **Parallel test execution** â€” Speed up large test suites
2. **HTML report output** â€” Alternative to markdown for web viewing
3. **Regression tracking** â€” Compare results across runs
4. **Custom evaluators** â€” Domain-specific evaluation criteria
5. **Multi-turn conversation tests** â€” Test conversation flow with session IDs

---

## Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAGTestSuiteFlow                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ @start()                                                    â”‚
â”‚ â””â”€> route_by_mode                                           â”‚
â”‚     â”œâ”€> "execute_only" â†’ load_from_csv                      â”‚
â”‚     â””â”€> others â†’ discover                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 1: Discovery & Generation                             â”‚
â”‚ â”œâ”€> discover_rag_data (DiscoveryCrew + RagQueryTool)        â”‚
â”‚ â”œâ”€> generate_prompt_suggestions (PromptGeneratorCrew)       â”‚
â”‚ â”œâ”€> check_prompt_only_exit                                  â”‚
â”‚ â”œâ”€> generate_test_cases (TestGenerationCrew)                â”‚
â”‚ â””â”€> check_generate_only_exit                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 2: Execution                                          â”‚
â”‚ â””â”€> execute_tests / execute_csv_tests (CrewRunnerTool +     â”‚
â”‚                                        EvaluatorTool)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 3: Evaluation & Reporting                             â”‚
â”‚ â”œâ”€> evaluate_results (EvaluationCrew)                       â”‚
â”‚ â””â”€> generate_report (ReportingCrew)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Known Issues

None currently. All unit tests pass. Integration testing validated Discovery and Test Generation phases. Execution phase requires a deployed target crew.

---

## Dependencies

```toml
[project]
requires-python = ">=3.10,<3.14"
dependencies = [
    "crewai[litellm,tools]==1.8.0",
    "python-dotenv>=1.0.0",
    "requests>=2.28.0",
    "pyyaml>=6.0.0",
    "pydantic>=2.0.0",
]
```

---

## Maintainers

- **Konecta Technology Incubation Center**
- Contact: tech@konecta.com
